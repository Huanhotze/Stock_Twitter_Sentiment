{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "9vEv3qSltial",
        "I7eXaZQQcHHL",
        "j1Cfo2z0cRNl",
        "aWnTSmBmeDBy",
        "fNQVfLEzuzEG",
        "VPCkq6xGfljS"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vEv3qSltial"
      },
      "source": [
        "####IMPORT SCRAP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGzClctkuqYi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import requests\n",
        "import csv\n",
        "from bs4 import BeautifulSoup as bs\n",
        "import lxml\n",
        "\n",
        "from urllib.request import urlopen\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
        "#Create Chrome Options - Necessary for Colab\n",
        "from selenium import webdriver\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "browser = webdriver.Chrome('chromedriver',options=options)\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "\n",
        "import unittest\n",
        "\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.common.action_chains import ActionChains\n",
        "\n",
        "import re\n",
        "from scrapy import Selector   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7eXaZQQcHHL"
      },
      "source": [
        "####IMPORT TWITTER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYIAFj98cHHO",
        "outputId": "30f5fada-d2de-4edb-a52c-629ef7b4c762"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import tweepy\n",
        "from textblob import TextBlob\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('fivethirtyeight')\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('stopwords')\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk.corpus import stopwords                            \n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.probability import FreqDist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1Cfo2z0cRNl"
      },
      "source": [
        "####IMPORT TELEGRAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlvFHsCFcRNn"
      },
      "outputs": [],
      "source": [
        "import telebot\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWnTSmBmeDBy"
      },
      "source": [
        "####TOKENS"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We add the tokens we will need for both Twitter and Telegram to some variables"
      ],
      "metadata": {
        "id": "nuKnAZSinTlj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kh9G-5Uuai6p"
      },
      "outputs": [],
      "source": [
        "#TWITTER\n",
        "consumerKey = YOUR TOKENS\n",
        "consumerSecret = YOUR TOKENS\n",
        "accessToken = YOUR TOKENS\n",
        "accessTokenSecret = YOUR TOKENS\n",
        "\n",
        "#TELEGRAM\n",
        "id_channel = YOUR CHANNEL\n",
        "telegram_token = YOUR TOKENS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNQVfLEzuzEG"
      },
      "source": [
        "##SCRAP"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We proceed to scrape the investing page where we extract the name and volume values from the table.\n",
        "\n",
        "The company name that gives us investing, is the one we will use later to search the Tweets in Telegram. "
      ],
      "metadata": {
        "id": "roanTPbQnfYX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0G5n693uuYw"
      },
      "outputs": [],
      "source": [
        "url = 'https://www.investing.com/equities/most-active-stocks?country=usa'\n",
        "session = requests.Session()\n",
        "request = session.get(url)\n",
        "soup = bs(request.content,'lxml')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRbIKc3fu8B1"
      },
      "outputs": [],
      "source": [
        "nombre = []; volumen = [];\n",
        "\n",
        "empresas = soup.find_all('td', class_='left bold plusIconTd elp', itemscope=\"\")\n",
        "volumen_empresas = soup.find_all('td', class_='turnover', itemscope=\"\")\n",
        "\n",
        "for empresa in empresas:\n",
        "    nombre.append(empresa.text)\n",
        "\n",
        "for EachPart in soup.select('td[class*=\"turnover\"]'):\n",
        "    volumen.append(EachPart.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We transform the extracted data into a dataframe to be able to work with the data in a simpler way."
      ],
      "metadata": {
        "id": "bDw6K0vdoCxl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltM5BMagu_1M"
      },
      "outputs": [],
      "source": [
        "nombre = [x.lower() for x in nombre]\n",
        "\n",
        "df_volumen = pd.DataFrame({\n",
        "    'Nombre': nombre,\n",
        "    'Volumen': volumen\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPCkq6xGfljS"
      },
      "source": [
        "##DATA FRAME"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We transform the data in the volume column so that it is a number and not a string."
      ],
      "metadata": {
        "id": "Gm7dJVg2oSRF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJ6K-Uwqg2NB"
      },
      "outputs": [],
      "source": [
        "df_volumen.dtypes\n",
        "df_volumen['Volumen'] = df_volumen['Volumen'].str[:-1] #Eliminamos la M de la columna Volumen\n",
        "df_volumen['Volumen'] = df_volumen['Volumen'].astype(float)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We download a table where each company name is related to its corresponding ticker, this will be useful when presenting the results in the Telegram message."
      ],
      "metadata": {
        "id": "eOsInFrpoXcY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9CUH38Bg90m"
      },
      "outputs": [],
      "source": [
        "\n",
        "url = 'https://raw.githubusercontent.com/Huanhotze/Stock_Twitter_Sentiment/main/stocks.xlsx'\n",
        "stocks = pd.read_excel(url)\n",
        "columns = ['Nombre', 'Ticker']\n",
        "stocks.columns = columns"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will merge the 2 tables resulting in a table with the name, volume and ticker.\n",
        "\n",
        "We will sort by value and finally we will be left with the 5 companies with the highest volume."
      ],
      "metadata": {
        "id": "Cfqgnh7KpZIb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N14HceZ1oZ7x"
      },
      "outputs": [],
      "source": [
        "df_tickers = pd.merge(df_volumen, \n",
        "                      stocks, \n",
        "                      on ='Nombre', \n",
        "                      how ='inner')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8FXgcAN3gG8"
      },
      "outputs": [],
      "source": [
        "df_tickers = df_tickers.sort_values(by=['Volumen'], ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKIdRKlBkJwi"
      },
      "outputs": [],
      "source": [
        "first_n_column  = df_tickers.iloc[:5]\n",
        "tickers = first_n_column.Ticker.values.tolist()\n",
        "names = first_n_column.Nombre.values.tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBQMY3LkxzTG"
      },
      "source": [
        "##LOGIN TWITTER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRPSzgDPrP4g"
      },
      "outputs": [],
      "source": [
        "#Create the authentication object\n",
        "authenticate = tweepy.OAuthHandler(consumerKey, consumerSecret)\n",
        "#Set the access token an the access token secret\n",
        "authenticate.set_access_token(accessToken, accessTokenSecret)\n",
        "#Create the API object\n",
        "api = tweepy.API(authenticate, wait_on_rate_limit=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MW6W7_WpH0R"
      },
      "source": [
        "##SCRAP FROM TWITTER"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function is in charge of extracting 1000 tweets from the term that is passed to it, in this case it will be the name of the company."
      ],
      "metadata": {
        "id": "reJk50kgpl1p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FL1Qj6BVySuU"
      },
      "outputs": [],
      "source": [
        "def scrap_tweets(search_term):\n",
        "  #Cursor object\n",
        "  tweets = tweepy.Cursor(api.search, q=search_term, lang='en', since='2022-10-21', tweet_mode= 'extended').items(1000)\n",
        "  #Store the tweets in a variable and get the full text\n",
        "  all_tweets = [tweet.full_text for tweet in tweets]\n",
        "  df = pd.DataFrame(all_tweets, columns=['Tweets'])  \n",
        "  return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMhDUOQeWXUr"
      },
      "source": [
        "##CREATE DATA FRAMES"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given a list, it will go through it and ask the previous function for the 1000 tweets that it will introduce in a dataframe called df_tweets_{stockName}."
      ],
      "metadata": {
        "id": "HVmtEpm9p3TJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCJAG6dE12oh"
      },
      "outputs": [],
      "source": [
        "for i, name in enumerate(names):\n",
        "  Dynamic_Variable_dfnum = f'df_tweets_{i}'\n",
        "  globals()[Dynamic_Variable_dfnum] = scrap_tweets(names[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWYdU64i3AnL"
      },
      "source": [
        "##CLEANING DATA"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following two functions will be in charge of cleaning the text of the dataframes with the tweets in order to leave in them only information of value and analyzable by the NLTK model."
      ],
      "metadata": {
        "id": "2ueO1SmEqdSs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZGswSw0pH0R"
      },
      "outputs": [],
      "source": [
        "def cleanData(twt, st):\n",
        "  twt = twt.to_string()\n",
        "  twt = twt.lower()\n",
        "  twt = re.sub(r'==.*?==+', ' ', twt)\n",
        "  twt = twt.replace('\\n', ' ')\n",
        "  twt = twt.replace('\\\\n', ' ')\n",
        "    twt = re.sub('@[A-Za-z0-9]+', ' ', twt) #Remove any string with a Hashtag\n",
        "  \n",
        "  return twt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Udda9PPbBB4O"
      },
      "outputs": [],
      "source": [
        "def token_stop_stemmed(text):\n",
        "  \n",
        "  tokenized_word = word_tokenize(text)\n",
        "  tokenized_word = [w for w in tokenized_word if w.isalpha()]\n",
        "  \n",
        "  stop_words = set(stopwords.words(\"english\"))\n",
        "  \n",
        "  filtered_word = [w for w in tokenized_word if w not in stop_words]\n",
        "  \n",
        "  ps = PorterStemmer()\n",
        "  stemmed_words=[]\n",
        "  for w in filtered_word:\n",
        "    stemmed_words.append(ps.stem(w))\n",
        "\n",
        "  return stemmed_words"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The loop will loop through each of the dataframes created earlier with the Twitter search.\n",
        "\n",
        "After passing through the first function it will return a string formed by the 1000 tweets clean of unnecessary symbols.\n",
        "\n",
        "Once the string is passed to the second function it returns a list of words."
      ],
      "metadata": {
        "id": "iT9btIQqrDj_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9kyFqO20UKJ"
      },
      "outputs": [],
      "source": [
        "for i, name in enumerate(names):\n",
        "  \n",
        "  dfnum_DV = f'df_tweets_{i}'\n",
        "  text_DV = f'tweets_text_{i}'\n",
        "  wordnum_DV = f'words_{i}'\n",
        "\n",
        "  globals()[text_DV] = cleanData(globals()[dfnum_DV], name)\n",
        "\n",
        "  globals()[wordnum_DV] = token_stop_stemmed(globals()[text_DV])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VapyR2nndfq9"
      },
      "source": [
        "##SENTIMENT ANALYSIS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6er1lLanJKN_"
      },
      "outputs": [],
      "source": [
        "def tweet_sentiment(text):\n",
        "  sid = SentimentIntensityAnalyzer()\n",
        "  stemmed_words_to_sentiment = ' '.join(text)\n",
        "  sentiment = sid.polarity_scores(stemmed_words_to_sentiment)\n",
        "  return sentiment"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With a loop like the previous ones, we will pass the word lists of each of the 5 stocks to the function in charge of analyzing the sentiment of the tweets. "
      ],
      "metadata": {
        "id": "zg7gpt5psg9N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64oRif0NuOJM"
      },
      "outputs": [],
      "source": [
        "for i, name in enumerate(names):\n",
        "  \n",
        "  wordnum_DV = f'words_{i}'\n",
        "  sentiment_DV = f'sentiment_{i}'\n",
        "\n",
        "  globals()[sentiment_DV] = tweet_sentiment(globals()[wordnum_DV])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the sentiment data that we have stored in a list for each of the stocks, we will extract the value corresponding to 'compound'."
      ],
      "metadata": {
        "id": "7L0UxsEUtn4i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJyOxXmrLIT2"
      },
      "outputs": [],
      "source": [
        "sentiment =[]\n",
        "\n",
        "for i, name in enumerate(names):\n",
        "  sentiment_DV = f'sentiment_{i}'\n",
        "\n",
        "  sentiment.append(globals()[sentiment_DV] ['compound'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNmWRTz5dn4p"
      },
      "source": [
        "##MANDAR MENSAJE TELEGRAM"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, with the sentiment data and ticker for each stock, we compose a message to send to Telegram. \n",
        "\n",
        "La funcion decidira con el dato de 'compound' si el sentimiento es positivo, negativo o neutro. "
      ],
      "metadata": {
        "id": "d1HSOP2AtroM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XOjoQ2yQTWi"
      },
      "outputs": [],
      "source": [
        "def text_telegram(stock, sentiment):\n",
        "  \n",
        "  if sentiment > 0.3:\n",
        "    text = f'👍  {stock} \\n'\n",
        "  elif sentiment < -0.3:\n",
        "    text = f'❌ {stock} \\n'\n",
        "  else:\n",
        "    text = f'↔ {stock} \\n'\n",
        "  \n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "giAId078dn4r"
      },
      "outputs": [],
      "source": [
        "bot = telebot.TeleBot(telegram_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nb030XXDRDDY"
      },
      "outputs": [],
      "source": [
        "today = datetime.datetime.now() \n",
        "text = f'📈 Most volatile stock sentimens for {today.year}/{today.month}/{today.day} \\n\\n'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El bucle generará un texto para cada stock segun su sentimiento y fianlmente se enviará a Telegram."
      ],
      "metadata": {
        "id": "mmwRvenVv_QR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7x9AAusQCq8"
      },
      "outputs": [],
      "source": [
        "for i, stock in enumerate(tickers):\n",
        "\n",
        "  text = text + text_telegram(stock, sentiment[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DcXoAZq5wto",
        "outputId": "a38cda83-46cd-4a0d-9f9d-639f6146aa64"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<telebot.types.Message at 0x7f9538b37250>"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ],
      "source": [
        "bot.send_message(id_channel, text)"
      ]
    }
  ]
}